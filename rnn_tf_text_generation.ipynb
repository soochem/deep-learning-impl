{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Text Generation x RNN\n",
    "* TensorFlow Tutorial을 참고하여 Custom RNN 모델을 작성\n",
    "* TF 1.15 적용\n",
    "* 셰익스피어 데이터셋을 이용해 텍스트 생성 문제\n",
    "    https://www.tensorflow.org/tutorials/text/text_generation?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 길이: 1115394자\n"
     ]
    }
   ],
   "source": [
    "# 문자열 디코딩\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# 텍스트 길이\n",
    "print ('텍스트의 길이: {}자'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 텍스트의 처음 250자를 살펴봅니다\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유 문자수 65개\n"
     ]
    }
   ],
   "source": [
    "# 파일의 고유 문자수를 출력합니다.\n",
    "vocab = sorted(set(text))\n",
    "print ('고유 문자수 {}개'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 처리\n",
    "    - 텍스트 벡터화 : 문자를 수치화해야 한다.\n",
    "    - 두개의 lookup table을 만들어 문자=>숫자 맵핑, 숫자=>문자 맵핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고유 문자(vocab)에서 인덱스로 맵핑\n",
    "# {'a':0, 'b':1, ...}\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "# ['a', 'b', ...]\n",
    "idx2char = np.array(vocab)\n",
    "# text => int(고유번호) 맵핑\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 각 문자에 대한 정수 표현\n",
    "print('{')\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 훈련 샘플과 타깃 생성 (tf.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text를 샘플 시퀀스로 나눈다. seq_lenth 길이로\n",
    "# 텍스트를 seq_length + 1 개의 청크로 나눈다.\n",
    "# ex. input: \"Hello\", seq_length: 4 => 입력 시퀀스 \"Hell\", 타깃 시퀀스 \"ello\"\n",
    "\n",
    "# 먼저 텍스트 벡터를 문자 인덱스의 스트림으로 변환\n",
    "# seq_length: 입력에 대해 원하는 문장의 최대 길이\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# 훈련 샘플 / 타깃 생성\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# for i in char_dataset.take(4):\n",
    "#     print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,)\n",
      "11153\n"
     ]
    }
   ],
   "source": [
    "print(text_as_int.shape)\n",
    "print(examples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-4b837e024fa8>:1: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "iterator = char_dataset.make_initializable_iterator()\n",
    "el = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(5):\n",
    "        print(idx2char[sess.run(el)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 생성 (배치 수 : examples_per_epoch)\n",
    "seq = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (101,), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "iterator = seq.make_initializable_iterator()\n",
    "el_seq = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(5):\n",
    "        item = sess.run(el_seq)\n",
    "        print(repr(''.join(idx2char[item])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11043\n",
      "alive! 11042\n"
     ]
    }
   ],
   "source": [
    "# 배치 개수 확인하기\n",
    "iterator = seq.make_initializable_iterator()\n",
    "el_seq = iterator.get_next()\n",
    "\n",
    "n_batches_seq = len(text)//(seq_length+1)\n",
    "print(n_batches_seq)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(n_batches_seq):\n",
    "        item = sess.run(el_seq)\n",
    "    print(\"alive! %d\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk 생성\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = seq.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_initializable_iterator()\n",
    "get_next = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    input_ex, target_ex = sess.run(get_next)\n",
    "    print(repr(''.join(idx2char[input_ex])))\n",
    "    print(repr(''.join(idx2char[target_ex])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'IteratorGetNext_3:0' shape=(100,) dtype=int64>,\n",
       " <tf.Tensor 'IteratorGetNext_3:1' shape=(100,) dtype=int64>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Step\n",
    "각 char에 매치되는 idx는 하나의 time step  \n",
    "time step 0의 입력으로 모델은 F의 idx 를 받고 다음 문자로 i의 idx를 예측  \n",
    "RNN은 현재 입력 문자 F 외에도 이전 time step의 context를 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0단계\n",
      "  입력: 18 ('F')\n",
      "  예상 출력: 47 ('i')\n",
      "   1단계\n",
      "  입력: 47 ('i')\n",
      "  예상 출력: 56 ('r')\n",
      "   2단계\n",
      "  입력: 56 ('r')\n",
      "  예상 출력: 57 ('s')\n",
      "   3단계\n",
      "  입력: 57 ('s')\n",
      "  예상 출력: 58 ('t')\n",
      "   4단계\n",
      "  입력: 58 ('t')\n",
      "  예상 출력: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_ex[:5], target_ex[:5])):\n",
    "    print(\"{:4d}단계\".format(i))\n",
    "    print(\"  입력: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  예상 출력: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 배치 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 데이터셋을 섞을 버퍼의 크기 (전체 시퀀스를 메모리에서 섞지 않음)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "alive! 171\n"
     ]
    }
   ],
   "source": [
    "# 배치 개수 확인하기\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "get_next = iterator.get_next()\n",
    "\n",
    "n_batches_dataset = n_batches_seq//64\n",
    "print(n_batches_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(n_batches_dataset):\n",
    "        item = sess.run(get_next)\n",
    "    print(\"alive! %d\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 설계\n",
    "![img.png](https://tensorflow.org/tutorials/text/images/text_generation_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. 케라스를 이용한 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size=len(vocab),\n",
    "                  embedding_dim=embedding_dim,\n",
    "                  rnn_units=rnn_units,\n",
    "                  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run model\n",
    "iterator_tf = dataset.make_initializable_iterator()\n",
    "get_next_tf = iterator_tf.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_tf.initializer)\n",
    "    input_ex, target_ex = sess.run(get_next_tf)\n",
    "    \n",
    "    example_batch_predictions = model(input_ex)\n",
    "    print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices,axis=-1)\n",
    "    sampled_indices = sampled_indices.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"입력: \\n\", repr(\"\".join(idx2char[input_ex[0]])))\n",
    "print()\n",
    "print(\"예측된 다음 문자: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer & Loss function\n",
    "이 모델은 로짓을 반환하기 때문에 from_logits 플래그를 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    example_batch_loss  = loss(target_ex, example_batch_predictions)\n",
    "    print(\"예측 배열 크기(shape): \", example_batch_predictions.shape, \" # (배치 크기, 시퀀스 길이, 어휘 사전 크기\")\n",
    "    print(\"스칼라 손실:          \", example_batch_loss.eval().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss=loss)\n",
    "# model.compile('rmsprop', 'mse', target_tensors=[iterator.get_next()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "# 체크포인트가 저장될 디렉토리\n",
    "# checkpoint_dir = './training_checkpoints'\n",
    "# # 체크포인트 파일 이름\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_prefix,\n",
    "#     save_weights_only=True)\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS)  #, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = np.zeros((sample_x.shape[0], HIDDEN_SIZE))\n",
    "    \n",
    "# _total_loss, _ = sess.run([total_loss, train],\n",
    "#                                feed_dict={rnn_input: sample_x,\n",
    "#                                           rnn_output: sample_y,\n",
    "#                                           initial_state: state})\n",
    "# print(f'[Epoch:{epoch+1}] loss: {_total_loss:<7.4}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. NumPy를 이용한 모델\n",
    "- Reference\n",
    "    - https://victorzhou.com/blog/intro-to-rnns/\n",
    "    - https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2tensor(char):\n",
    "    tensor = np.zeros((vocab_size, ))\n",
    "    tensor[char2idx[char]] = 1\n",
    "    return tensor\n",
    "\n",
    "def line2tensor(line):\n",
    "    tensor = np.zeros((len(line), vocab_size, ))\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][char2idx[char]] = 1\n",
    "    return tensor\n",
    "\n",
    "def idx2tensor(idx):\n",
    "    tensor = np.zeros((vocab_size, ))\n",
    "    tensor[idx] = 1\n",
    "    return tensor\n",
    "\n",
    "def idxs2tensor(idxs):\n",
    "    tensor = np.zeros((len(idxs), vocab_size, ))\n",
    "    for li, letter in enumerate(idxs):\n",
    "        tensor[li][letter] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [0, 1, 0, 0, ...]\n",
    "char2tensor('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_dataset = dataset.make_one_shot_iterator()        # 한번에 배치 만들어줌\n",
    "get_next_dataset = iterator_dataset.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    input_ex_batch, target_ex_batch = sess.run(get_next_dataset)\n",
    "            \n",
    "    # BATCH_SIZE 만큼\n",
    "    for i, (input_ex, target_ex) in enumerate(zip(input_ex_batch, target_ex_batch)):\n",
    "        # tensor는 한 문장 (100, 65, 1)\n",
    "        input_tensor = idxs2tensor(input_ex)\n",
    "        target_tensor = idxs2tensor(target_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'IteratorGetNext_6:0' shape=(64, 100) dtype=int64>,\n",
       " <tf.Tensor 'IteratorGetNext_6:1' shape=(64, 100) dtype=int64>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53, 44,  0, ..., 52, 43, 43],\n",
       "       [61, 47, 58, ..., 15, 21, 26],\n",
       "       [47, 57,  1, ..., 40, 43, 39],\n",
       "       ...,\n",
       "       [46, 47, 57, ...,  1, 57, 46],\n",
       "       [ 1, 39, 58, ..., 53,  1, 15],\n",
       "       [ 1, 46, 39, ..., 52,  2,  1]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ex_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 65)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = idxs2tensor(input_ex)\n",
    "print(input_tensor.shape)\n",
    "# print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "        85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([50, 47, 52, 45, 40, 56, 53, 49, 43,  6,  0, 13, 52, 42,  1, 57, 58,\n",
       "        56, 39, 47, 45, 46, 58,  1, 39, 51,  1, 52, 53, 58, 46, 47, 52, 45,\n",
       "        10,  1, 40, 59, 58,  1, 61, 46, 39, 58, 43,  5, 43, 56,  1, 21,  1,\n",
       "        40, 43,  6,  0, 26, 53, 56,  1, 21,  1, 52, 53, 56,  1, 39, 52, 63,\n",
       "         1, 51, 39, 52,  1, 58, 46, 39, 58,  1, 40, 59, 58,  1, 51, 39, 52,\n",
       "         1, 47, 57,  0, 35, 47, 58, 46,  1, 52, 53, 58, 46, 47, 52]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(input_tensor==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Simple RNN Cell\n",
    "- Many-to-many\n",
    "- Tx = Ty (fixed-length)\n",
    "![image](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-same-ltr.png?2790431b32050b34b80011afead1f232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.layers import tanh, softmax\n",
    "\n",
    "class RNNCell:    \n",
    "    def __init__(self\n",
    "                , input_dim=None\n",
    "                , output_dim=None\n",
    "                , hidden_size=32\n",
    "                , initial_state=None\n",
    "                , activation=tanh\n",
    "                , seed=1\n",
    "                ):\n",
    "        '''\n",
    "        ================ parameters =================\n",
    "        input  : inputs\n",
    "        shape  : (sequence_length, embedding_size, )\n",
    "        =============================================\n",
    "        output : y \n",
    "        shape  : (inputs.shape[0]  # sequence_length\n",
    "                , output_dim       # embedding_size\n",
    "                , )\n",
    "        '''\n",
    "\n",
    "        # init\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if output_dim is None:\n",
    "            self.output_dim=hidden_size\n",
    "        if initial_state is None:\n",
    "            self.initial_state = np.zeros((hidden_size, ))\n",
    "        else:\n",
    "            self.initial_state = np.reshape(initial_state, (hidden_size, ))\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        # weights\n",
    "        self.Wxh = np.random.normal(size=(hidden_size, input_dim)).round(3)\n",
    "        self.Whh = np.random.normal(size=(hidden_size, hidden_size)).round(3)\n",
    "        self.Why = np.random.normal(size=(output_dim,  hidden_size)).round(3)\n",
    "        # bias\n",
    "        self.bh = np.zeros((hidden_size, )).round(3)\n",
    "        self.by = np.zeros((output_dim,  )).round(3)\n",
    "        self.activation = activation\n",
    "        \n",
    "        # back prop\n",
    "        self.last_inputs = None\n",
    "        self.last_hs = {0: initial_state}\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # tanh를 적용하는 순간 ht가 1로 수렴한다. (30 넘는 값)\n",
    "        # word embedding이 필요한듯\n",
    "        # 초기화 문제? yes ... normal(음수 필요), glorot_uniform(?)\n",
    "    \n",
    "        ht = self.initial_state # hidden state\n",
    "        outputs = np.zeros((inputs.shape[0], self.output_dim)) # (100, 65, )\n",
    "        states = np.zeros((inputs.shape[0], self.hidden_size))\n",
    "        \n",
    "        # back prop\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = {0: ht}\n",
    "        \n",
    "        for i, xt in enumerate(inputs): # 모든 글자에 대해 (input: 'hello', xt: 'h')\n",
    "            # hidden state\n",
    "            ht = self.activation(self.Wxh @ xt + self.Whh @ ht + self.bh)\n",
    "            self.last_hs[i+1] = ht\n",
    "            # predicted output y\n",
    "            y = (self.Why @ ht) + self.by\n",
    "            outputs[i] = y\n",
    "            states[i] = ht\n",
    "        \n",
    "        return outputs, states  # 100개의 y, ht\n",
    "    \n",
    "    def backprop(self, dy, learning_rate=1e-2):\n",
    "        # dy : (output_dim x 1)\n",
    "        time = len(self.last_inputs)\n",
    "        \n",
    "        # calculate dL/dWhy & dL/dby\n",
    "        # (output_dim x hidden_size) = (output_dim x 1) * (1 x hidden_size)\n",
    "        dWhy = np.expand_dims(dy, 1) @ np.expand_dims(self.last_hs[time], 0)\n",
    "        dby = dy\n",
    "        # init\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        # (hidden_size x ) = (hidden_size x output_dim) x (output_dim x )\n",
    "        dh = self.Why.T @ dy\n",
    "        \n",
    "        for t in reversed(range(time)):\n",
    "            \n",
    "            last_h = np.expand_dims(self.last_hs[t], 0)\n",
    "            last_input = np.expand_dims(self.last_inputs[t], 0)\n",
    "#             print( last_h.shape )\n",
    "#             print( last_input.shape )\n",
    "        \n",
    "            dh_raw = None\n",
    "            if self.activation == tanh:\n",
    "                dh_raw = (1 - self.last_hs[t+1]**2)\n",
    "            elif self.activation == sigmoid:\n",
    "                dh_raw = (1 - self.last_hs[t+1]) * self.last_hs[t+1]\n",
    "            else: \n",
    "                return\n",
    "            # (hs, ) * (hs, )\n",
    "            dh_raw *= dh\n",
    "            \n",
    "            # dh_raw : (hs, )\n",
    "            dbh += dh_raw\n",
    "            # (hs x hs) = (hs x 1) x (1 x hs)\n",
    "            dWhh += np.expand_dims(dh_raw, 1) @ last_h\n",
    "            # (hs x input_dim) = (hs x 1) x (1 x input_dim)\n",
    "            dWxh += np.expand_dims(dh_raw, 1) @ last_input\n",
    "            dh = self.Whh @ dh_raw  # ??\n",
    "        \n",
    "        # Update weights and biases using GD\n",
    "        self.Whh -= learning_rate * dWhh\n",
    "        self.Wxh -= learning_rate * dWxh\n",
    "        self.Why -= learning_rate * dWhy\n",
    "        self.bh -= learning_rate * dbh\n",
    "        self.by -= learning_rate * dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== output ========\n",
      "shape: (100, 65)\n",
      "[48 10 60 20 61 58 43 10 38 54 60 13 11 43  6 47 31 58 13 57 61 48 38 58\n",
      " 54 62  9 54 11 32 32 27 20  3 48 51 50  2 11 60 11 51 29 57 58 21 36 49\n",
      " 33 36  2  8  5 37 60 60 59 36 57 57 18 50 50 32  7 43 62 59 45 63 27 36\n",
      " 60 47 26 49 55 59 63 55 36 54 31 37 30  9 28 27 33 17  1 58 46 21 21 18\n",
      " 31 18 11  6]\n"
     ]
    }
   ],
   "source": [
    "# 문장 하나에 대한 테스트 (100, 65)\n",
    "output_dim = 65\n",
    "hidden_size = 128\n",
    "hidden_state = np.zeros((hidden_size, ))\n",
    "\n",
    "rnn = RNNCell(input_dim=vocab_size  # feature 수\n",
    "           , output_dim=output_dim\n",
    "           , hidden_size=hidden_size\n",
    "           , initial_state=hidden_state)\n",
    "\n",
    "output, hidden_state = rnn.forward(input_tensor)\n",
    "\n",
    "print(\"======== output ========\")\n",
    "print(\"shape: %s\" % str(output.shape))\n",
    "print(np.argmax(output, 1))\n",
    "# print([np.argmax(item, axis=0) for item in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== backprop ========\n",
      "done....\n"
     ]
    }
   ],
   "source": [
    "# BACKPROP\n",
    "print(\"======== backprop ========\")\n",
    "# calculate dL/dy : Loss에 대한 y의 gradient\n",
    "dLdy = sum(output - target_tensor)\n",
    "# dLdy[target_tensor] -= 1  # ??\n",
    "rnn.backprop(dLdy)\n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - np.log([item[y[i]] for (i, item) in enumerate(y_hat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "loss 312300.408\n",
      "accuracy 0.016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.layers import softmax\n",
    "\n",
    "# hyperparameters\n",
    "input_dim = vocab_size\n",
    "output_dim = vocab_size  # embedding_size = 65\n",
    "hidden_size = vocab_size\n",
    "epochs = 1\n",
    "n_batches = n_batches_dataset\n",
    "backprop = False\n",
    "# init\n",
    "hidden_state = np.zeros((hidden_size, ))\n",
    "max_output = np.zeros((BATCH_SIZE, seq_length, ), int)\n",
    "losses = list()\n",
    "# dataset\n",
    "iterator_dataset = dataset.make_initializable_iterator() # get_next로 하나씩 가져옴\n",
    "# iterator_dataset = dataset.make_one_shot_iterator()        # 한번에 배치 만들어줌\n",
    "get_next_dataset = iterator_dataset.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        sess.run(iterator_dataset.initializer)\n",
    "        num_correct = 0\n",
    "        loss = 0.\n",
    "        \n",
    "        rnn = RNNCell(input_dim=input_dim\n",
    "               , output_dim=output_dim\n",
    "               , hidden_size=hidden_size\n",
    "               , initial_state=hidden_state)\n",
    "            \n",
    "        for n in range(n_batches):\n",
    "            \n",
    "            # load dataset batch\n",
    "            # input_ex_batch: (n_batches, batch_size, seq_length)\n",
    "            input_ex_batch, target_ex_batch = sess.run(get_next_dataset)\n",
    "            \n",
    "            \n",
    "            for i, (input_ex, target_ex) in enumerate(zip(input_ex_batch, target_ex_batch)): \n",
    "                \n",
    "                # tensor는 한 문장 (100, 65, 1)\n",
    "                input_tensor = idxs2tensor(input_ex)\n",
    "                \n",
    "                # FORWARD\n",
    "                outputs, states = rnn.forward(input_tensor)\n",
    "                outputs = softmax(outputs)\n",
    "                # (100, 65, 1) 중에 가장 가능성 높은 것만 선택 -> (100, )\n",
    "                # argmax default(flatten), axis 0(행), axis 1(열)\n",
    "                max_output[i] = np.argmax(outputs, 1)\n",
    "\n",
    "                # loss & accuracy -- working\n",
    "                # MSE\n",
    "                # E(y_true, y_pred) = 1/n * sum(y_pred - y_pred)\n",
    "#                 loss += 1/seq_length * [item[target_ex[i]] - 1 for (i, item) in enumerate(outputs)]\n",
    "                loss += sum(cross_entropy(outputs, target_ex)) / seq_length\n",
    "                num_correct += np.sum(max_output[i] == target_ex)\n",
    "\n",
    "                # BACKPROP\n",
    "                if backprop:\n",
    "                    # calculate dL/dy : Loss에 대한 y의 gradient\n",
    "                    dLdy = (outputs - target_tensor)[-1]\n",
    "                    rnn.backprop(dLdy)\n",
    "\n",
    "        print(\"EPOCH %d\" % e)\n",
    "        print(\"loss %.3f\" % loss)\n",
    "        print(\"accuracy %.3f\" % (num_correct / (n_batches * BATCH_SIZE * seq_length)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.log([out[target_ex[i]] for (i, out) in enumerate(outputs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 65)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i3zF!GnfGi!L&GnykMUoy!zra$ecNA!duSPASDTZSb$$T$\\nqazaGduhR jvpSrO.cpd3jrdq;bSRRO,IW-NSo!RVR$RNN.OfIlG'\""
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = \"\"\n",
    "for item in max_output[0]:\n",
    "    ans += idx2char[item]\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번외 테스트\n",
    "- array sequence\n",
    "- short string (\"hello\" => \"ello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 5x4 data\n",
    "input_test = np.array([ \n",
    "           [1,2,3,4]\n",
    "         , [2,3,4,5]\n",
    "         , [3,4,5,9]\n",
    "         , [4,5,9,3]\n",
    "         , [5,9,3,1] ])\n",
    "\n",
    "target_test = np.array([\n",
    "           [2,3,4,5]\n",
    "         , [3,4,5,9]\n",
    "         , [4,5,9,3]\n",
    "         , [5,9,3,1]\n",
    "         , [9,3,1,1] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.375     ],\n",
       "       [0.25      , 0.14285714, 0.16666667, 0.5       ],\n",
       "       [0.5       , 0.28571429, 0.33333333, 1.        ],\n",
       "       [0.75      , 0.42857143, 1.        , 0.25      ],\n",
       "       [1.        , 1.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = scaler.fit_transform(input_test)\n",
    "input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.375     , 0.5       ],\n",
       "       [0.14285714, 0.16666667, 0.5       , 1.        ],\n",
       "       [0.28571429, 0.33333333, 1.        , 0.25      ],\n",
       "       [0.42857143, 1.        , 0.25      , 0.        ],\n",
       "       [1.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test = scaler.fit_transform(target_test)\n",
    "target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-23abb423d87e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# FORWARD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# loss & accuracy -- working\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-267-fb6333c49ec9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 모든 글자에 대해 (input: 'hello', xt: 'h')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWxh\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mxt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhh\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mht\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# predicted output y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "input_dim = 1\n",
    "output_dim = 1 #4\n",
    "hidden_size = 1 #3\n",
    "epochs = 1\n",
    "# init\n",
    "hidden_state = np.zeros((hidden_size, ))\n",
    "# max_output = np.zeros((BATCH_SIZE, seq_length, ), int)\n",
    "losses = list()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    rnn_test = RNNCell(input_dim=input_dim\n",
    "                   , output_dim=output_dim\n",
    "                   , hidden_size=hidden_size\n",
    "                   , initial_state=hidden_state)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        loss = 0\n",
    "        num_correct = 0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(zip(input_test, target_test)):\n",
    "\n",
    "            # FORWARD\n",
    "            outputs, states = rnn_test.forward(inputs)\n",
    "\n",
    "            # loss & accuracy -- working\n",
    "            # E(y_true, y_pred) = -y_true * log(y_pred)  \n",
    "            #                   => -sum( E (y_true, y_pred) )\n",
    "            loss += .5 * sum([(out[0][0] - targets[j])**2 for j, out in enumerate(outputs)])\n",
    "            num_correct += np.sum(outputs == targets)\n",
    "            \n",
    "            # BACKPROP\n",
    "            # calculate dL/dy : Loss에 대한 y의 gradient\n",
    "            \n",
    "            print(f\"output:{outputs}\")\n",
    "            print(f\"target:{targets}\")\n",
    "            \n",
    "            print(f\"output:{outputs[-1]}\")\n",
    "            print(f\"target:{targets[-1]}\")\n",
    "            \n",
    "            dLdy = outputs[-1] - targets[-1]\n",
    "            rnn_test.backprop(dLdy)\n",
    "\n",
    "        print(\"loss %.3f\" % loss)\n",
    "        print(\"accuracy %.3f\" % (num_correct / (i+1)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.37490094e-12, 2.20687376e-13, 2.11974007e-11, ...,\n",
       "        7.52281899e-15, 3.64914921e-11, 7.00585116e-14],\n",
       "       [5.65793220e-14, 2.01931417e-10, 2.65762810e-10, ...,\n",
       "        1.52667898e-10, 7.88839626e-14, 2.37505521e-07],\n",
       "       [9.56872016e-15, 1.43173296e-10, 1.46095722e-08, ...,\n",
       "        5.49537259e-16, 5.74108245e-09, 1.25803372e-10],\n",
       "       ...,\n",
       "       [1.73762316e-08, 1.96644774e-14, 8.11756036e-12, ...,\n",
       "        5.01853863e-18, 5.20082673e-09, 4.74739202e-11],\n",
       "       [1.56023482e-14, 6.72361408e-08, 9.39449015e-15, ...,\n",
       "        1.78614663e-10, 7.03859159e-19, 4.93649767e-12],\n",
       "       [9.48138002e-12, 1.06062815e-11, 2.99409159e-14, ...,\n",
       "        2.17395820e-13, 8.86660691e-13, 1.86445926e-11]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. TensorFlow를 이용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-test",
   "language": "python",
   "name": "tf-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
