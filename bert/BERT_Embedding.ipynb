{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFtZYQgETPTv"
   },
   "source": [
    "# BERT 임베딩  \n",
    "[BERT Word Embeddings Tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)  \n",
    "[BERT Word Embeddings 튜토리얼 번역](https://codlingual.tistory.com/m/98)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91y9pA9hTJ-N"
   },
   "outputs": [],
   "source": [
    "!  pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 910,
     "status": "ok",
     "timestamp": 1602984719140,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "tjEsh5RITBAv"
   },
   "outputs": [],
   "source": [
    "# Pre-trained BERT 불러오기\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1308,
     "status": "ok",
     "timestamp": 1602984719557,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "m7wKDudETVRE",
    "outputId": "82c06d03-0f15-43a6-ea74-1f3241f8c143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'cl', '##s', ']', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[', 'sep', ']']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS]\" + text + \"[SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "elapsed": 1298,
     "status": "ok",
     "timestamp": 1602984719558,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "hYhNTd4mTYLk",
    "outputId": "d4299430-70a4-4e5f-f388-971c4c99906e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1288,
     "status": "ok",
     "timestamp": 1602984719559,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "PVqvcdaeTcsO"
   },
   "outputs": [],
   "source": [
    "# Segment ID\n",
    "\n",
    "segments_ids = [1] * len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7626,
     "status": "ok",
     "timestamp": 1602984725901,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "sqREHCrQVE0h"
   },
   "outputs": [],
   "source": [
    "# - 인풋 문장이 하나면 segment ID 모두 1로 주면 됨 (인풋 문장 + 마지막 [SEP] 까지 1 주기)\n",
    "# - 인풋 문장이 두개면 첫 문장은 0, 다음 문장은 1로 줘서 구분하기 \n",
    "\n",
    "# Extracting Embeddings\n",
    "\n",
    "# Python list를 PyTorch tensor로 변환하기 \n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# 미리 학습된 모델(가중치) 불러오기\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 모델 \"evaluation\" 모드 : feed-forward operation\n",
    "model.eval()\n",
    "\n",
    "# 우린 forward pass만 하지 [오차역전파해서 가중치 업데이트, 학습시키기] 이런거 안 하니까 no_grad()\n",
    "# no_grad() 쓰면 계산 더 빠름 \n",
    "\n",
    "# 각 레이어의 은닉상태 확인하기\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OujAZI3cVlVC"
   },
   "source": [
    "* Encoded_layers\n",
    "\n",
    "    1) layer 개수 : len(encoded_layers) # 12개\n",
    "\n",
    "    2) batch 개수 : len(encoded_layers[0]) # 1개\n",
    "\n",
    "    3) 단어/토큰 개수 :  len(encoded_layers[0][0]) # 22개\n",
    "\n",
    "    4) 은닉 상태 차원(hidden units/features) : len(encoded_layers[0][0][0]) # 768개\n",
    "\n",
    "\n",
    "* 현재 encoded_layers의 차원 \n",
    "\n",
    "    [layer 개수, batch 개수, token 개수, feature 개수]\n",
    "\n",
    "* 바꾸고 싶은 차원 \n",
    "\n",
    "    [token 개수, layer 개수, feature 개수]\n",
    "\n",
    "\n",
    "- encoded_layers 자체는 Python list \n",
    "\n",
    "- 각 layer 안은 torch tensor로 이루어짐 \n",
    "\n",
    "- encoded_layers[0].size()  # torch.Size([1,22,768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7585,
     "status": "ok",
     "timestamp": 1602984725920,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "NxQSPWysTs4c",
    "outputId": "e40cf6fe-88ff-4293-f8aa-353ea797cd82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 22, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12개의 layer를 합쳐서 하나의 큰 tensor로 만들기 \n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "token_embeddings.size() # torch.Size([12, 1, 22, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7452,
     "status": "ok",
     "timestamp": 1602984725926,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "JPU_Hw3CVNVr",
    "outputId": "f3d68797-f909-4417-d9a1-0ba3c19a73c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 22, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch 차원 없애기\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings.size()  # torch.Size([12, 22, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7316,
     "status": "ok",
     "timestamp": 1602984725930,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "Jou2O-BDVMXX",
    "outputId": "a0eb2bed-3aab-4e56-aef5-bcbe1db0632e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 12, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지금까지 하면 token_embeddings 차원은 \n",
    "# [layer 개수, token 개수, feature 개수]\n",
    "# 여기서 layer 개수와 token 개수의 자리만 바꾸면 됨 \n",
    "\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "token_embeddings.size()  # torch.Size([22, 12, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA6yhiAhVW2L"
   },
   "source": [
    "## 은닉상태로부터 단어/문장 벡터 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMDfYtcmUqIM"
   },
   "source": [
    "### 1. 단어(토큰) 벡터 만들기\n",
    "\n",
    "\n",
    "1) 맨 마지막 4개 레이어 이어붙이기(concatenate)\n",
    "\n",
    "- 각 벡터의 길이는 4*768 = 3072 \n",
    "\n",
    "- 4는 레이어 개수, 768은 feature 개수 \n",
    "\n",
    "- 각 토큰을 3072 길이의 벡터로 나타냈는데, 총 22개의 토큰이 있음\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 6873,
     "status": "ok",
     "timestamp": 1602984725933,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "Aq2oNd-sUpzb",
    "outputId": "bc26746a-dec6-4fac-bfef-2c77613e0990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 3072\n"
     ]
    }
   ],
   "source": [
    "token_vecs_cat = []\n",
    "\n",
    "# token_embeddings : [22,12,768]\n",
    "# token : [12,768]\n",
    "for token in token_embeddings :\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "    \n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "# Shape is: 22 x 3072\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7B1qmsqUxb3"
   },
   "source": [
    "2) 맨 마지막 4개 레이어 합치기(sum)\n",
    "\n",
    "-  합치기만 했으니 한 토큰을 나타내는 벡터 길이는 여전히 768\n",
    "\n",
    "- 토큰이 총 22개 있으니 최종 shape는 22*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 6578,
     "status": "ok",
     "timestamp": 1602984725937,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "SncI_M4AUzhI",
    "outputId": "f36c569a-6169-44a5-8c36-d9720e89cd40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "token_vecs_sum = []\n",
    "\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    token_vecs_sum.append(sum_vec)\n",
    "    \n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT8A0_xTU1-f"
   },
   "source": [
    "## 2. 문장 벡터 만들기\n",
    "\n",
    "- 마지막 레이어에서 모든 토큰의 은닉상태 평균 구하기 \n",
    "\n",
    "- 평균 구했으니 벡터 길이는 여전히 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1602984725941,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "1wT4oXnRU4yg"
   },
   "outputs": [],
   "source": [
    "# encoded_layers : [12*1*22*768]\n",
    "# token_vecs : [22*768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# sentence_embedding : [768]\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "elapsed": 6132,
     "status": "ok",
     "timestamp": 1602984725943,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "CmBp6IRBU6Dz",
    "outputId": "745f189b-055d-44ed-b528-7db577f58df7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 동음이의어 벡터 확인하기 \n",
    "for i, token_str in enumerate(tokenized_text):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 5985,
     "status": "ok",
     "timestamp": 1602984725945,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "Br3bt91ZU9_p",
    "outputId": "508a8a03-5712-4c53-be9c-6409ae0557b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    tensor([ 2.1319, -2.1413, -1.6260,  0.8638,  3.3173])\n",
      "bank robber   tensor([ 1.1868, -1.5298, -1.3770,  1.0648,  3.1446])\n",
      "river bank    tensor([ 1.1295, -1.4725, -0.7296, -0.0901,  2.4970])\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs_sum[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 5849,
     "status": "ok",
     "timestamp": 1602984725947,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "v-Qb86xzUiPg",
    "outputId": "aee2ba13-f45a-45de-c995-753ce93344ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.95\n",
      "Vector similarity for *different* meanings:  0.68\n"
     ]
    }
   ],
   "source": [
    "# 각 임베딩의 유사성 계산하기 \n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# 다른 의미의 bank 임베딩 비교 \n",
    "# \"bank robber\" vs \"river bank\" (different meanings)\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# 같은 의미의 bank 임베딩 비교\n",
    "# \"bank robber\" vs \"bank vault\" (same meaning)\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 5741,
     "status": "ok",
     "timestamp": 1602984725949,
     "user": {
      "displayName": "Sumin Seo",
      "photoUrl": "https://lh5.googleusercontent.com/-J-ssOdyao5o/AAAAAAAAAAI/AAAAAAAAALs/yvNXFP_w01c/s64/photo.jpg",
      "userId": "16707507274762580913"
     },
     "user_tz": -540
    },
    "id": "lbOpl8sgVBNb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMr/T4RZPwZMOE7ncWXAMFW",
   "collapsed_sections": [],
   "name": "BERT_Embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
